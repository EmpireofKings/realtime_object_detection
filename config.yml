---
### USB Webcam on PC ###
#video_input: 0              # Input Must be OpenCV readable
### USB Webcam on TX2 ###
video_input: 1
### Onboard camera on TX2 ### (need: apt-get install libxine2)
#video_input: "nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720,format=(string)I420, framerate=(fraction)30/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink"

force_gpu_compatible: False # If True with visualize False, speed up. Forces all CPU tensors to be allocated with Cuda pinned memory.
visualize: True             # True: Show result image. False: Without image show.
vis_worker: False           # True: Visualization run on process. (With visuzalize:True)
max_vis_fps: 0              # >=1: Limit of show fps. 0: No limit - means try to spend full machine power for visualization. (With visualize:True.)
vis_text: True              # Display fps on result image. (With visualize:True.)
max_frames: 5000            # Quit when frames done. (With viualize:False)
width: 600                  # Camera width.
height: 600                 # Camera height.
fps_interval: 5             # FPS console out interval and FPS stream length.
det_interval: 100           # intervall [frames] to print detections to console
det_th: 0.5                 # detection threshold for det_intervall
split_model: True           # Splits Model into a GPU and CPU session (currently only works for ssd_mobilenet_v1)
log_device: False           # Logs GPU / CPU device placement
allow_memory_growth: True   # limits memory allocation to the actual needs
ssd_shape: 300              # used for the split model algorithm
                            # currently only supports ssd networks trained on 300x300 and 600x600 input
debug_mode: False           # Show FPS spike value

# model_type supports 'nms_v1', 'nms_v2'.

# ssd_mobilenet_v1_coco_2017_11_17
#model_type: 'nms_v1'
#model_path: 'models/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb'
#label_path: 'models/labels/mscoco_label_map.pbtxt'
#num_classes: 90

# ssd_mobilenet_v1_2017_11_17 own data
#model_type: 'nms_v1'
#model_path: 'models/ssd_mobilenet_v1_roadsign_2017_11_17/roadsign_frozen_inference_graph_v1_204k.pb'
#label_path: 'models/ssd_mobilenet_v1_roadsign_2017_11_17/roadsign_label_map.pbtxt'
#num_classes: 4

# ssd_mobilenet_v1_coco_2018_01_28 4104 nodes
model_type: 'nms_v2'
model_path: 'models/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb'
label_path: 'models/labels/mscoco_label_map.pbtxt'
num_classes: 90

# ssd_mobilenet_v2_coco_2018_03_29
#model_type: 'nms_v2'
#model_path: 'models/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb'
#label_path: 'models/labels/mscoco_label_map.pbtxt'
#num_classes: 90

# ssdlite_mobilenet_v2_coco_2018_05_09
#model_type: 'nms_v2'
#model_path: 'models/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb'
#label_path: 'models/labels/mscoco_label_map.pbtxt'
#num_classes: 90

# ssd_inception_v2_coco_2018_01_28
#model_type: 'nms_v2'
#model_path: 'models/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'
#label_path: 'models/labels/mscoco_label_map.pbtxt'
#num_classes: 90


# TensorRT
# NEW DIR will be created.
# ./data/ - download model. it has checkpoint.
# ./logs/ - Graph diagram for tensorboard.
# NEW FROZEN FILE will be created.
# ./data/ssd_mobilenet_v1_coco_2017_11_17_trt_FP16.pb
# MODEL DOWNLOAD
# ssd_mobilenet_v1_coco_2017_11_17
# ssd_inception_v2_coco_2017_11_17
# ssd_mobilenet_v1_coco_2018_01_28
# ssd_mobilenet_v2_coco_2018_03_29
# ssdlite_mobilenet_v2_coco_2018_05_09
# ssd_inception_v2_coco_2018_01_28
# PRECISION MODEL
# https://devtalk.nvidia.com/default/topic/1023708/gpu-accelerated-libraries/fp16-support-on-gtx-1060-and-1080/
# FP16 is Tesla P100, Quadro GP100, and Jetson TX1/TX2.
# INT8 is cc6.1 or cc7.0
# FP32 for other devices.

#model_type: 'trt_v1'
#precision_model: 'FP32'     # 'FP32', 'FP16', 'INT8'
#model: 'ssdlite_mobilenet_v2_coco_2018_05_09'
#label_path: 'models/labels/mscoco_label_map.pbtxt'
#num_classes: 90
